{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Load Dataset Paths**\n",
    "\n",
    "Set the paths for training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/input/brain-tumor-mri-dataset/Training'\n",
    "test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n",
    "\n",
    "# Helper function to load file paths and labels\n",
    "def load_data(directory):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for fold in os.listdir(directory):\n",
    "        foldpath = os.path.join(directory, fold)\n",
    "        for f in os.listdir(foldpath):\n",
    "            filepaths.append(os.path.join(foldpath, f))\n",
    "            labels.append(fold)\n",
    "    return pd.DataFrame(data={'filepaths': filepaths, 'labels': labels})\n",
    "\n",
    "train_df = load_data(train_dir)\n",
    "ts_df = load_data(test_dir)\n",
    "\n",
    "# Split test data for validation and test sets\n",
    "valid_df, test_df = train_test_split(ts_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Data Augmentation and Generators**\n",
    "\n",
    "Prepare the data for training using augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for training data\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=20, width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "# Only rescale for validation and test data\n",
    "val_test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Create Generators\n",
    "train_gen = train_datagen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=(224, 224),\n",
    "                                              color_mode='rgb', class_mode='categorical', batch_size=32)\n",
    "\n",
    "valid_gen = val_test_datagen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=(224, 224),\n",
    "                                                 color_mode='rgb', class_mode='categorical', batch_size=32)\n",
    "\n",
    "test_gen = val_test_datagen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=(224, 224),\n",
    "                                                color_mode='rgb', class_mode='categorical', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Model Architecture**\n",
    "\n",
    "Define the architecture of the convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(4, activation='softmax')  # Output layer for 4 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Model Training with Early Stopping and Model Checkpoint**\n",
    "\n",
    "Train the model using the training data, with callbacks to stop training early if the validation loss does not improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('brain_tumor_best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(train_gen, validation_data=valid_gen, epochs=30, callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Training History Visualization**\n",
    "\n",
    "Visualize the training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Model Evaluation**\n",
    "\n",
    "Evaluate the model's performance on the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on each dataset\n",
    "train_loss, train_acc = model.evaluate(train_gen, verbose=0)\n",
    "valid_loss, valid_acc = model.evaluate(valid_gen, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(test_gen, verbose=0)\n",
    "\n",
    "# Plot Loss and Accuracy Comparison\n",
    "categories = ['Train', 'Validation', 'Test']\n",
    "loss_values = [train_loss, valid_loss, test_loss]\n",
    "accuracy_values = [train_acc, valid_acc, test_acc]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(categories, loss_values, color=['blue', 'orange', 'green'])\n",
    "ax[0].set_title('Loss Comparison')\n",
    "ax[0].set_xlabel('Dataset')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "ax[1].bar(categories, accuracy_values, color=['blue', 'orange', 'green'])\n",
    "ax[1].set_title('Accuracy Comparison')\n",
    "ax[1].set_xlabel('Dataset')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Prediction on Test Images and Model Saving**\n",
    "\n",
    "Load the best model, generate predictions, and save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and evaluate on test data\n",
    "best_model = model.load_weights('brain_tumor_best_model.h5')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = np.argmax(best_model.predict(test_gen), axis=1)\n",
    "true_labels = test_gen.classes\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels, predictions, target_names=test_gen.class_indices.keys()))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('final_brain_tumor_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: Predict Disease on New Image**\n",
    "\n",
    "Create a function to predict the class of a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Predict the class\n",
    "    prediction = model.predict(img_array)\n",
    "    class_index = np.argmax(prediction)\n",
    "    class_labels = list(test_gen.class_indices.keys())\n",
    "    return class_labels[class_index], prediction[0][class_index]\n",
    "\n",
    "# Test the function with an image from the test directory\n",
    "sample_image = test_df.sample(1).filepaths.values[0]\n",
    "predicted_class, confidence = predict_image(sample_image, model)\n",
    "print(f\"Predicted Class: {predicted_class}, Confidence: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
